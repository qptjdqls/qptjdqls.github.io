---
layout: post
title: CLT
date: 2022-08-11 11:59:59 +0900
categories: DL
permalink: /6
---

# LLN

## 원본 글:
## [Law of large numbers](https://en.wikipedia.org/wiki/Law_of_large_numbers)

1. 큰 수의 약한 법칙 <br/>
서로 독립인 확률변수 $X _1, X _2, \dots$ 이 모두 기댓값 $\mu$ 인 동일한 확률분포를 따를 때, 어떤 작은 양의 수 $\epsilon$ 에 대해서도 다음이 성립한다. <br/>
$\underset{n\to\infty}{\lim} Pr(\|\bar{X} _n -\mu\|<\epsilon)=1$ <br/>
기댓값이 같다면, 분산이 변하여도 LLN 이 성립할 수 있다. Chebyshev 에 의하면 $n$ 이 무한으로 갈 때 $n$ 개의 평균'의' 분산이 0으로 수렴하면 해당 법칙이 성립함을 보였다. 예를 들어 zero mean gaussian distirbution 에서 분산이 $2n/\log(n+1)$ 로 나타날 때, 분산의 합은 asymptotic to $n^2/\log n$. 따라서 평균의 분산은 $1/\log n$ 으로 0으로 수렴함으로 LLN 이 성립한다. <br/>
2. 큰 수의 강한 법칙 <br/>
The strong law of large numbers (also called Kolmogorv's law) sates that the sample average converages almost surely to the expected value. <br/>
$Pr(\underset{n\to\infty}{\lim} \bar{X} _n = \mu) = 1$ <br/>

---
# CLT

## 원본 글:
## [https://namu.wiki/w](https://namu.wiki/w/%EC%A4%91%EC%8B%AC%EA%B7%B9%ED%95%9C%EC%A0%95%EB%A6%AC)
## [https://angeloyeo.github.io](https://angeloyeo.github.io/2020/09/15/CLT_meaning.html)

* 독립항등분포 (i.i.d.) 를 따르는 확률변수 $X _1, X _2, \dots , X _n$ 에 대해, 각각의 평균은 $E(X _i)=\mu$ 이고 각각의 표준편차는 $\sigma$ 라 하자. $\xi _n=\frac{\sum _{i=1}^n X _i -n\mu}{\sqrt{n}\sigma}$ 라 둘 때, $\xi _n$ 은 표준정규분포로 분포수렴한다.
* 표본 평균의 분포는 정규 분포에 근사하게 된다.
* 모집단의 모양이 어떻든 관계없이 중심극한 정리는 성립한다.
* 심지어는 표본을 추출하는 모집단이 서로 독립적이라면 여러 모집단에서 추출한 표본이더라도 표본 평균의 분포는 정규분포에 근사하게 된다 (Lyapunov CLT)
---
1. 적률생성함수를 이용하는 증명 (적률생성함수가 존재하는 확률변수일 경우만 유효) <br/>
$\mathbb{E}(\bar{X})=\frac{1}{n}\{\mathbb{E}(X _1)+\dots + \mathbb{E}(X _n)\}$ <br/>
$=\frac{1}{n}\times n\mathbb{E}(X)=\mu$<br/>
$Var(\bar{X})=\frac{1}{n^2}\{Var(X _1)+\dots +Var(X _n)\}$ <br/>
$=\frac{1}{n^2}\times n Var(X)=\frac{\sigma^2}{n}$ <br/>
$\therefore \sigma(\bar{X})=\frac{\sigma}{\sqrt{n}}$ <br/>
([적률생성함수](https://namu.wiki/w/%EC%A0%81%EB%A5%A0%EC%83%9D%EC%84%B1%ED%95%A8%EC%88%98) $M _X (t)= \mathbb{E}[e^{tX}]$) <br/>
$M _{\frac{\sqrt{n}(\bar{X}-\mu)}{\sigma}}=\mathbb{E}(exp(\frac{(X _1 -\mu)+\dots + (X _n - \mu)}{\sigma \sqrt{n}}t))$ <br/>
$= \mathbb{E}(exp(\frac{X-\mu}{\sigma \sqrt{n}}t))\dots \mathbb{E}(exp(\frac{X _n -\mu}{\sigma \sqrt{n}}t))$ <br/>
$=\{\mathbb{E}(exp(\frac{X-\mu}{\sigma \sqrt{n}}t))\}^n$<br/>
$=\{M _{\frac{X-\mu}{\sigma}}(\frac{t}{\sqrt{n}})\}^n$ <br/>
$\therefore \underset{n\to \infty}{\lim} M _{\frac{\sqrt{n}(\bar{X}-\mu)}{\sigma}}(t)$ <br/>
$=exp(\underset{n\to\infty}{\lim} n \ln M _{\frac{\bar{X}-\mu}{\sigma}} (\frac{t}{\sqrt{n}}))$ <br/>
여기서 $h=\frac{1}{\sqrt{n}}$ 이라 하면 $n\to\infty$ 일 때 $h\to\infty$ 이므로 <br/>
$=exp(\underset{h\to 0}{\lim}\frac{\ln M _{\frac{X-\mu}{\sigma}}(th)}{h^2})$ <br/>
여기서 $\underset{h\to 0}{\lim} M _{\frac{X-\mu}{\sigma}}(th)=1$ 이므로 로피탈의 정리에 의해 <br/>
$=exp(\underset{h\to 0}{\lim}\frac{t M' _{\frac{X-\mu}{\sigma}}(th)}{2 h M _{\frac{X-\mu}{\sigma}}(th)})$ <br/>
$=exp(\frac{t}{2} \underset{h\to\infty}{\lim} \frac{M' _{\frac{X-\mu}{\sigma}}(th)-0}{h})$ <br/>
여기서 $\underset{h\to 0}{\lim}M' _{\frac{X-\mu}{\sigma}}(th)=0$ 이므로 미분계수의 정의에 의해 <br/>
$=exp(\frac{t}{2}\underset{h\to 0}{\lim}\frac{M' _{\frac{X-\mu}{\sigma}}(t\times h)-M' _{\frac{X-\mu}{\sigma}}(t\times 0)}{h})$ <br/>
$=exp(\frac{t}{2}\times t M'' _{\frac{X-\mu}{\sigma}}(t\times 0))$ <br/>
여기서 $M'' _{\frac{X-\mu}{\sigma}}(0)=\mathbb{E}((\frac{X-\mu}{\sigma})^2)$ <br/>
$=Var(\frac{X-\mu}{\sigma})+\{\mathbb{E}(\frac{X-\mu}{\sigma})\}^2$ <br/>
$=1+0^2=1$ <br/>
$\therefore \underset{n\to\infty}{\lim}M _{\frac{\sqrt{n}(\bar{X}-\mu)}{\sigma}}(t)=e^{\frac{t^2}{2}}$ 로 표준정규분포의 적률생성함수와 같은 형태이다. 즉 $n\to\infty$일 때 $\frac{\sqrt{n}(\bar{X}-\mu)}{\sigma}\sim N(0,1)$ 이므로 $\bar{X}\sim N(\mu, \frac{\sigma^2}{n})$ 이다. <br/>


2. 특성함수를 이용하는 증명 <br/>

* 확률 변수의 합과 확률 밀도함수의 convolution <br/>

독립적인 r.v. $X, Y$ 의 확률질량함수를 $m _1(x), m _2(x)$ 라고 하자. 이 때 , $Z=X+Y$ 로 정의되는 새로운 r.v. 를 생각해 보자. <br/>
$P(Z=z)=\sum _{k=-\infty}^{\infty} P(X=k)P(Y=z-k)$ <br/>
따라서, 독립적인 r.v. $X, Y$ 의 합인 $Z$ 의 확률질량함수는 다음과 같이 나타낼 수 있다. <br/>
$m _3(j) = \sum _{k} m _1(k) m _2(j-k)\ for\ j=\dots,-1,-1,0,1,2,\dots$ <br/>
위와 같은 논리를 이용하여 $n$ 개의 독립적인 r.v. $X _1, X _2, \dots, X _n$ 에 대하여 $n$ 개의 r.v. 의 합을 $S _n$ 으로 나타낼 수 있으며, 이것은 $S _n \ S _{n-1}+ X _n$ 이라고 표현할 수 있다. 따라서 무수한 independent r.v. 의 합에 대한 확률질량함수를 convolution 을 이용해 구할 수 있다. <br/>
더 나아가 continous r.v. $X,Y$ 에 대해서 다음이 성립한다. $X,Y$ 의 확률밀도함수를 각각 $f(x),g(x)$ 라고 하자. 이 때, $f(x),g(x)$ 는 모든 실수에 대해 정의되있다고 할 때, $X+Y$ 의 확률밀도함수는 두 확률밀도함수의 convolution 으로 표현할 수 있다. <br/>
$(f*g)(z)=\int _{-\infty}^{\infty} f(z-y)g(y)dy=\int _{-\infty}^{\infty} g(z-x)f(x)dx$ <br/>

* Characteristic function <br/>

신호처리에서 푸리에 변환은 시간 함수를 시간 도메인에서 주파수 도메인으로 변환시켜주는 역할을 하고, 역 푸리에 변환은 주파수 도메인의 함수를 시간 도메인으로 변환시켜주는 역할을 한다. 또, 시간 영역의 함수와 그 쌍인 주파수 영역의 함수는 1:1 매핑을 이룬다. <br/>
통계학에서는 이러한 관계를 이용하여 characteristic function 이라는 개념을 만들었는데, 이것은 수학적으로 역 푸리에 변환과 동일한 식을 가지며, 푸리에 변환이 가지고 있는 성질들을 그대로 이어받으며, 통계학에서만 활용되는 pdf 의 moment 와 관련된 개념과 연관시켜 사용되기도 한다.<br/>
확률통계학에서 임의의 r.v. 에대한 characteristic function 은 다음과 같이 정의한다. <br/>
임의의 확률 밀도 함수 $f _X(x)$ 에 대한 characteristic function 은 <br/>
$\phi _X(t)=E[e^{jtx}]=\int _{-\infty}^{\infty} e^{jtx} f _X(x) dx$ <br/>
characteristic function 의 성질 중 CLT 의 증명에 필요한 것을 꼽자면 다음과 같다. <br/>

  1. 각각의 r.v. 들은 고유(unique)의 characteristic function 을 가진다. 즉 r.v. 하나와 그에 상응하는 characterstic funciton 은 1:1 mapping 관계를 가진다.
  2. 서로 독립인 $p$ 개의 r.v. $X _1, \dots, X_p$ 에 대해 다음이 성립한다. 위에서 독립 r.v. 의 합의 pdf 는 convolutoin 으로 나타난다고 했다. 따라서 characteristic function 의 domain 에서 각각의 독립적인 r.v. 들의 합의 characteristic function 은 각각 독립적인 r.v. 들의 characerstic function 의 곱으로 나타낼 수 있다.

한편 맥클로린 급수를 이용하면 $e^{jtx}$ 를 다음과 같이 풀어서 생각할 수 있다. <br/>
$e^{jtx}=\sum _{k=1}^{\infty}\frac{(jtx)^k}{k!}=1+jtx+\frac{(jtx)^2}{2!}+\dots=1+jtx+\frac{(jtx)^2}{2!}+O(t^2)$ <br/>
이 때 $O(t^2)$는 네 번째 이후의 항을 합쳐서 생각한 것이다. 그러면 characteristic function 을 계산하는 것은 pdf 의 moment 를 계산하는 것과 연관되게 된다. 임의의 확률 밀도 함수 $f _Y(y) $ 에 대하여, <br/>
$\phi _Y(t) = \int _{-\infty}^{\infty} e^{jty} f_Y(t) dy=\int_{-\infty}^{\infty}\{1+jty-\frac{t^2}{2}y^2 + O(t^2)\}f _Y(y)dy$ <br/>
$=\int _{-\infty}^{\infty} f _Y(y)dy+jt \int _{-\infty}^{\infty} y f _Y(y)dy - \frac{t^2}{2} \int _{-\infty}^{\infty} y^2 f _Y(y)dy + O(t^2)$ <br/>
$1+jt \mathbb{E}[y] - \frac{t^2}{2} \mathbb{E}[y^2]+O(t^2)$ <br/>
와 같다. <br/>

* Central limit theorem 의 증명 <br/>

$\mathbb{E}(\bar{X})=\frac{1}{N}\{\mathbb{E}(X _1)+\dots + \mathbb{E}(X _N)\}$ <br/>
$=\frac{1}{N}\times N\mathbb{E}(X)=\mu$<br/>
$Var(\bar{X})=\frac{1}{N^2}\{Var(X _1)+\dots +Var(X _N)\}$ <br/>
이 때, $\bar{X} _N$ 을 정규화 시킨 변수를 $Z _N$ 이라고 하면, <br/>
$Z _N = \frac{\bar{X} _N - \mu}{(\frac{\sigma}{\sqrt{N}})}=\frac{N\bar{X} _N - N \mu}{\sqrt{N}\mu}=\frac{N\bar{X} _N - N\mu}{\sqrt{N}\sigma}=\frac{\sum _{i=1}^N (X _i-\mu)}{\sqrt{N} \sigma}$ <br/>
$=\sum _{i=1}^N(\frac{X _i -\mu}{\sqrt{N}\sigma})=\sum _{i=1}^N \frac{y _i}{\sqrt{N}}$ <br/>
여기서 $y _i =(X _i-\mu)/\sigma$ 이므로 $\mathbb{E}[y _i]=0$ 이고 $Var[y _i] = Var[X _i] / \sigma^2 = 1$ 이다. <br/>
$\phi _Y(y)=1+0-\frac{t^2}{2}+O(t^2)=1-\frac{t^2}{2}+O(t^2)$ <br/>
이때, $Z _N = \sum _{i=1}^N \frac{y _i}{\sqrt{N}}$ 와 같이 $Z _N$ 은 $Y$ 의 합으로 구성되어 있고, 이것은 $f _Y(y)$ 의 컨볼루션으로 나타날 것이며, 다시 한 번 characteristic function 의 domain 에서는 characteristic function 의 곱으로 표현할 수 있다. 따라서 다음이 성립한다. <br/>
$\phi _{Z _N}(t)=[\phi _Y(t/\sqrt{N})]^N = [1-\frac{t^2}{2N}+O(\frac{t^2}{N})]^N$ <br/>
이 때 $N$ 이 무한히 커진다면 $O(\frac{t^2}{N})$ 는 $\frac{t^2}{2N}$ 보다 더 빨리 0으로 수렴할 것이다. 따라서, 극한은 다음으로 수렴하게 된다. <br/>
$\underset{N\to\infty}{\lim}\phi _{Z _N} (t)=\underset{N\to\infty}{\lim}[1-\frac{t^2}{2N}]^N=e^{-t^2/2}$ <br/>
이는 표준정규분포의 characteristic function 과 일치하고, characteristic function 은 r.v. 와 1:1 매핑 관계를 가지고 있으므로 표본 평균의 분포는 가우시안 분포를 가지게 된다. <br/>