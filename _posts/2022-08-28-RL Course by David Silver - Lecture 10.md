---
layout: post
title: RL Course by David Silver - Lecture 10
date: 2022-08-28 11:59:59 +0900
categories: RL
permalink: /23
---

# [RL Course by David Silver - Lecture 10](https://www.youtube.com/watch?v=kZ_AUmFcZtk&list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ&index=10) /  [visible ver.](https://www.youtube.com/watch?v=N1LKLc6ufGY)



## Classic Games

---

### State of the art

**Why study classic games?** <br>
![](/public/img/2022-08-28-RLCoursebyDavidSilver-Lecture10/1.png){: width="50%" height="50%"}{: .center} <br>

---

### Game Theory

**Optimality in gamess** <br>
What is the optimal policy $\pi^i$ for $i$th player? <br>
If all other players fix their polices $\pi^{-i}$ <br>
Best response $\pi _{\*}^i(\pi^{-i})$ is optimal policy against those polices <br>
Nash equilibrium is a joint policy for all players <br>
$\pi^i = \pi _{\*}^i(\pi^{{-i}})$ <br>
Such that every player's policy is a best response <br>
i.e. no player would choose to deviate from *Nash* <br>

**Singl-agent and self-play reinforcement learning** <br>
Best response is solution to single-agent RL problem <br>
-Other players become part of the environment <br>
-Game is reduced to an MDP <br>
-Best response is optimal policy for this MDP <br>
Nash equailibrium is fixed-point of self-play RL <br>
-Experience is generated by playing games between agents <br>
$a _1\sim \pi^1, a _2 \sim \pi^2, \dots$ <br>
-Each agent learns best response to other players <br>
-One player's policy determines another player's environment <br>
-All players are adapting to each other <br>

**Two-player zero-sum games** <br>
We will focus on a special class of games: <br>
-A two-player game has two (alternating) players <br>
--We will name player 1 white and player 2 black <br>
-A zero sum game has equal and opposite rewards for black and white <br>
$R^1+R^2=0$ <br>
We consider methods for finding Nash equilibria in these games <br>
-Game tree search (i.e. planning) <br>
-Self-play reinforcement learning <br>

**Perfect and Imperfect Information Games** <br>
A perfect information or Markov game is fully observed <br>
-Chess <br>
-Checkers <br>
-Othello <br>
-Backgammon <br>
-Go <br>
An imperfect information game is partiall yobserved <br>
-Scrabble <br>
-Poker <br>
We focus first on perfect information games <br>

---

### Minimax Search
![](/public/img/2022-08-28-RLCoursebyDavidSilver-Lecture10/2.png){: width="50%" height="50%"}{: .center} <br>

**Minimax search** <br>
![](/public/img/2022-08-28-RLCoursebyDavidSilver-Lecture10/3.png){: width="50%" height="50%"}{: .center} <br>

**Value function in minimax search** <br>
![](/public/img/2022-08-28-RLCoursebyDavidSilver-Lecture10/4.png){: width="50%" height="50%"}{: .center} <br>

**Binary-linear value function** <br>
![](/public/img/2022-08-28-RLCoursebyDavidSilver-Lecture10/5.png){: width="50%" height="50%"}{: .center} <br>

---

### Self-play reinforcement learning (41:38)
![](/public/img/2022-08-28-RLCoursebyDavidSilver-Lecture10/6.png){: width="50%" height="50%"}{: .center} <br>

**Policy improvement with afterstates** <br>
![](/public/img/2022-08-28-RLCoursebyDavidSilver-Lecture10/7.png){: width="50%" height="50%"}{: .center} <br>

**Self-play TD in Othello: Logistello** <br>
![](/public/img/2022-08-28-RLCoursebyDavidSilver-Lecture10/8.png){: width="50%" height="50%"}{: .center} <br>

**Reinforcment learning in Logistello** <br>
![](/public/img/2022-08-28-RLCoursebyDavidSilver-Lecture10/9.png){: width="50%" height="50%"}{: .center} <br>

**TD Gammon** <br>
![](/public/img/2022-08-28-RLCoursebyDavidSilver-Lecture10/10.png){: width="50%" height="50%"}{: .center} <br>

**Self-play TD in Backgammon: TD-Gammon** <br>
![](/public/img/2022-08-28-RLCoursebyDavidSilver-Lecture10/11.png){: width="50%" height="50%"}{: .center} <br>

---

### Combining reinforcement learning and minimax search

**Simple TD (1:02:25)** <br>
![](/public/img/2022-08-28-RLCoursebyDavidSilver-Lecture10/12.png){: width="50%" height="50%"}{: .center} <br>

**Simple TD: Results** <br>
![](/public/img/2022-08-28-RLCoursebyDavidSilver-Lecture10/13.png){: width="50%" height="50%"}{: .center} <br>

**TD Root** <br>
![](/public/img/2022-08-28-RLCoursebyDavidSilver-Lecture10/14.png){: width="50%" height="50%"}{: .center} <br>

**TD Leaf** <br>
![](/public/img/2022-08-28-RLCoursebyDavidSilver-Lecture10/15.png){: width="50%" height="50%"}{: .center} <br>

**TreeStrap** <br>
![](/public/img/2022-08-28-RLCoursebyDavidSilver-Lecture10/16.png){: width="50%" height="50%"}{: .center} <br>

**Simulation-Based Search** <br>
![](/public/img/2022-08-28-RLCoursebyDavidSilver-Lecture10/17.png){: width="50%" height="50%"}{: .center} <br>

---

### Reinforcement learning in imperfect-information games

**Game-tree search in imperfect information games** <br>
![](/public/img/2022-08-28-RLCoursebyDavidSilver-Lecture10/18.png){: width="50%" height="50%"}{: .center} <br>

**Solution methods for imperfect information games** <br>
![](/public/img/2022-08-28-RLCoursebyDavidSilver-Lecture10/19.png){: width="50%" height="50%"}{: .center} <br>

***Smooth UCT Search*** <br>
![](/public/img/2022-08-28-RLCoursebyDavidSilver-Lecture10/20.png){: width="50%" height="50%"}{: .center} <br>

---

### Conclusions

**RL in games: a successful recipe** <br>
![](/public/img/2022-08-28-RLCoursebyDavidSilver-Lecture10/21.png){: width="50%" height="50%"}{: .center} <br>

---

