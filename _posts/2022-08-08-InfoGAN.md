---
layout: post
title: InfoGAN
date: 2022-08-08 11:59:59 +0900
categories: DL
permalink: /1
---

# InfoGAN

## 원본 글:
## [jonathan-hui.medium.com](https://jonathan-hui.medium.com/gan-cgan-infogan-using-labels-to-improve-gan-8ba4de5f9c3d)
## [https://jaejunyoo.blogspot.com/](https://jaejunyoo.blogspot.com/2017/03/infogan-1.html)

## 1. Using labels to import GAN
In GAN, traning models are non-trivial. It can take extra help from the labes to make the model performing better. <br/>
일반 GAN 은 generator의 input 으로 들어가는 continous noise vector $z$ 에 아무런 제약을 주지 않기 때문에 생성 모델이 매우 꼬인(highly entangled) 방식으로 representation 을 학습할 여지가 있다. 따라서 $z$ 의 individual dimensions 가 데이터의 어떤 주요한 의미와 굳이 대응될 이유가 없다. 또한 데이터가 고차원으로 갈수록 representation 이 "복잡하지 않은" 혹은 "꼬이지 않은" 방식으로 학습될 가능성이 낮아진다.

## 2. CGAN
Vanilla GAN 에서는 생성할 데이터의 mode 에 대한 제어를 할 방법이 없다. CGAN 은 label $y$ 를 인풋에 같이 넣어줌으로써 generator 가 해당하는 라벨에 맞는 이미지를 생성하기를 바라는 방법.

> $\underset{G}{\min}\underset{D}{\max}V(D,G)=\mathbb{E} _{x\sim p _{data}(x)}[\log D(x\|y)] + \mathbb{E} _{z\sim p_z (z)}[\log (1-D(G(z\|y)))]$

![](/public/img/2022-08-08-InfoGAN/CGAN.png){: width="50%" height="50%"}{: .center}

CGAN 의 구조에서 확인할 수 있듯이 real image & label pair 가 필요하다. label 은 (MNIST 에서) $y\approx~U(0,9)$ 에 국한 된 것이 아니라 stroke size 등의 prior 를 모두 사용하면 된다.

## 3. InfoGAN
CGAN 과 같이 주어진 label $y$ 만 활용하는 것이 아니라, distriminator 를 통해 이러한 latent feature 들을 모두 추출할 수 있도록 한다. <br/>
CGAN 과 동일한 방법으로 single feature $c\sim U(0,9)$ 를 샘플링 하고 인풋으로 함께 들어간다. Discriminator 는 discrimination 외에도 $Q(c|x)$ 를 $c$ 와 유사한 분포를 출력하도록 학습되고, 방법은 $c$ 와 $G(z,c)$ 의 mutual information 이 최대가 되도록 하는 것이다. 

> $\underset{G}{\min}\underset{D}{\max}V_{infoGAN}(D,G)=V_{GAN}(D,G)-\lambda I(c;G(z,c))$
>> $V _{GAN}=\mathbb{E} _{x\sim p _{data}(x)}[\log D(x)]+\mathbb{E} _{z\sim p _z(z)}[\log (1-D(G(z,c)))]$



![](/public/img/2022-08-08-InfoGAN/InfoGAN.png){: width="50%" height="50%"}{: .center}

input noise vector 를 더 이상 압축되지 않는 noise 에 해당하는 $z$ 와 data distribution 의 semantic feature 에 해당하는 latent code $c$ 두 부분으로 나누어 넣는다. <br/>
기존 GAN 에서는 다른 제약이 없기 때문에 생성 모델이 $G(z,c)$ 이 되어도 $P _G(x|c)=P _G(x)$ 를 만족하는 solution 을 찾음으로 $c$ 를 간단히 무시해 버릴 수 있다. InfoGAN 은 이를 막기 위해 $I(c;G(z,c))$ 를 높게 유지함으로써 제약조건을 준다. <br/>

## 4. Variational Mutual Information Maximization
앞서 말한 $I(c;G(z,c))$ 를 구하기 위해서는 posterior distribution $P(c\|x)$ 를 계산해야하고, $p(x)$ 가 intractable 하기 때문에 직접적인 값을 얻기 어렵다.<br/>
따라서 이를 우회하기 위해 variational information maximization 이라는 lower bounding technique 가 사용된다. $P(c|x)$ 를 직접 구하기는 어려우니 $Q(c|x)$ 라는 auxiliary distribution 을 사용하여 $P(c|x)$ 를 근사하는 방식으로 계산이 편한 lower bound 를 구한다.<br/>

>$I(c;G(z,c)) = H(c)-H(c\|G(z,c))$<br/>
>$=\mathbb{E} _{x\sim G(z,c)}[\mathbb{E} _{c'\sim P(c\|x)} [\log P(c'\|x)]]+H(c)$<br/> 

>> conditional entropy 정의에 주의 <br/>
>> $H(X\|Y)=-\int _{X,Y} f(x,y)\log f(x\|y)dxdy$<br/>

> $=\mathbb{E} _{x\sim G(z,c)}[D _{KL}(P(\cdot \|x)\|\|Q(\cdot \|x)) + \mathbb{E} _{c'\sim P(c\|x)}[\log Q(c' \|x)]] + H(c)$<br/>

>> $D _{KL}(P(\cdot \|x)\|\|Q(\cdot \|x)) \geq 0$ <br/>

> $\geq \mathbb{E} _{x\sim G(z,c)}[\mathbb{E} _{c'\sim P(c\|x)}[\log Q(c' \|x)]] + H(c)$<br/>
> $=L _I(G,Q)$<br/>

이제 $Q(c\|x)$ 를 사용하여 $P(c\|x)$ 를 직접 계산하는 것을 우회할 수 있다. 하지만 $c'\sim P(c\|x)$ 로부터 sampling 을 해야하는 문제가 남아있는데 이는 [law of total expectation](https://en.wikipedia.org/wiki/Law_of_total_expectation) 이라는 proposition 을 이용해 해결할 수 있다. <br/>
(law of total probability / expectation / variance ...) <br/>


>$\mathbb{E}(X) = \mathbb{E}(\mathbb{E}(X\|Y))$

>Lemma 5.1 $\mathbb{E} _{x\sim X, y\sim Y\|x}[f(x,y)]=\mathbb{E} _{x\sim X, y\sim Y\|x, x' \sim X\|y}[f(x',y)]$

>$L _I(G,Q)=\mathbb{E} _{x\sim G(z,c)} [\mathbb{E} _{c' \sim P(c\|x)}[\log Q(c' \|x)]]+H(c)$ <br/>
>$=\mathbb{E} _{c\sim P(c), x\sim G(z,c)}[\log Q(c\|x)]+H(c)$ <br/>

이제 $L _i(G,Q)$ 를 monte carlo simulation 으로 근사하기 쉬워진 것을 확인할 수 있다. 게다가 $Q\to P$ 일 때, $L _I(G,Q)$ 가 $H(c)$ 로 최대값을 갖기 때문에 하한이 타이트 해지고 최대 상호 정보량 (maximul mutual information)을 갖게 할 수 있다. <br/>
실제 implementation 에서는 auxiliary distribution $Q$ 가 neural network 를 이용해 $Q(c\|x;\theta)$ parameterize 되고 보통 $Q(c|x)$ 를 결과로 내야하는 마지막 fully connected layer 를 제외하고는 모든 layer 에서 $D$ 와 같은 weight 를 공유하기 때문에 실제로 GAN 에 비해 계산량 증가가 미미하다고 한다.
